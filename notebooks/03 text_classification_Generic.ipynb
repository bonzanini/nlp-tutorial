{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text Classification\n",
    "\n",
    "There are three main types of classification:\n",
    "\n",
    "- Binary: Two mutually exclusive categories (e.g., Spam detection)\n",
    "- Multiclass: More than 2 mutually exclusive categories (e.g., Language detection)\n",
    "- Multilabel: Non-mutually exclusive categories (e.g., movie genres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary text classification problem\n",
    "\n",
    "We will adress the binary problem of detecting Sport related documents vs any other type of documents. In order to do this we will create an artificial (and very small collection).\n",
    "\n",
    "- Define a set of labelled documents that will be our *training dataset*. These are the documents the classifier will learn from in order to categorise future _unseen_ documents\n",
    "\n",
    "- Define a set of labelled documents that will be our *testing dataset*. These will be the \"unseen\" documents that the classifier will predict (without having being trained with them)\n",
    "\n",
    "- Represent our training and testing documents\n",
    "\n",
    "- Train the classifier based on the training data\n",
    "\n",
    "- Predict the labels for the testing documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Train and test data. Both the full documents and their labels (\"Sports\" vs \"Non Sports\")\n",
    "train_data = ['Football: a great sport', 'The referee has been very bad this season', 'Our team scored 5 goals', 'I love tenis',\n",
    "              'Politics is in decline in the UK', 'Brexit means Brexit', 'The parlament wants to create new legislation',\n",
    "              'I so want to travel the world']\n",
    "train_labels = [\"Sports\",\"Sports\",\"Sports\",\"Sports\", \"Non Sports\", \"Non Sports\", \"Non Sports\", \"Non Sports\"]\n",
    "\n",
    "test_data = ['Swimming is a great sport', \n",
    "             'A lot of policy changes will happen after Brexit', \n",
    "             'The table tenis team will travel to the UK soon for the European Championship']\n",
    "test_labels = [\"Sports\",\"Non Sports\",\"Sports\"]\n",
    "\n",
    "# Representation of the data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorised_train_data = vectorizer.fit_transform(train_data)\n",
    "vectorised_test_data = vectorizer.transform(test_data)\n",
    "\n",
    "# Train the classifier given the training data\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(vectorised_train_data, train_labels)\n",
    "\n",
    "# Predict the labels for the test documents (not used for training)\n",
    "print(classifier.predict(vectorised_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations, you have built your first text classifier!!\n",
    "\n",
    "However, the third case is wrongly classified. Why do you think that might be?\n",
    "\n",
    "- Matching problems (e.g., \"car\" is different than \"Cars\")\n",
    "- Cases never seen before (e.g., the classifier has never seen the word \"table\")\n",
    "- \"Spurious\" correlations and bias (\"car\" appears only in the positive category)\n",
    "\n",
    "Lets look into how we are representing our documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Function to show the feature weights of a document (to be explained later)\n",
    "def feature_values(doc, representer):\n",
    "    doc_representation = representer.transform([doc])\n",
    "    features = representer.get_feature_names()\n",
    "    return [(features[index], doc_representation[0, index]) for index in doc_representation.nonzero()[1]]\n",
    "\n",
    "pprint([feature_values(doc, vectorizer) for doc in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets try again, with stop-word removal this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the list of (english) stop-words from nltk\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# Represent, train, predict\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "vectorised_train_data = vectorizer.fit_transform(train_data)\n",
    "vectorised_test_data = vectorizer.transform(test_data)\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(vectorised_train_data, train_labels)\n",
    "\n",
    "print(classifier.predict(vectorised_test_data))\n",
    "# Expected: [Sports, Non Sports, Sports]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class classification problem\n",
    "\n",
    "We will adress the multi-class problem of detecting the language of a sentence based on 3 mutually exclusive languages (e.g., Spanish, English and French). For the sake of this example, we assume those are the only 3 languages that the documents can have. As before, we will create an artificial (and very small collection) with similar steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Artificial (and small) dataset. Spanish,English,French texts\n",
    "train_data = ['PyCon es una gran conferencia', 'Aprendizaje automatico esta listo para dominar el mundo dentro de poco',\n",
    "             'This is a great conference with a lot of amazing talks', 'AI will dominate the world in the near future',\n",
    "             'Dix chiffres por resumer le feuilleton de la loi travail']\n",
    "train_labels = [\"SP\", \"SP\", \"EN\", \"EN\", \"FR\"]\n",
    "\n",
    "test_data = ['Estoy preparandome para dominar las olimpiadas', 'Me gustaria mucho aprender el lenguage de programacion Scala',\n",
    "             'Machine Learning is amazing','Hola a todos']\n",
    "test_labels = [\"SP\", \"SP\", \"EN\", \"SP\"]\n",
    "\n",
    "# Represent\n",
    "vectorizer = TfidfVectorizer() # Note, we are not doing stop-word removal. Stop words could be beneficial in this problems\n",
    "vectorised_train_data = vectorizer.fit_transform(train_data)\n",
    "vectorised_test_data = vectorizer.transform(test_data)\n",
    "\n",
    "# Train\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(vectorised_train_data, train_labels)\n",
    "\n",
    "# Predict\n",
    "predictions = classifier.predict(vectorised_test_data)\n",
    "pprint(predictions)\n",
    "# Expected: [SP, SP, EN, SP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mmm, the last case is wrong. Can you guess why?\n",
    "\n",
    "- Can we learn from never seen cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You have just build a nice Language detection system!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label Problem\n",
    "\n",
    "We will adress the multi-label problem of labeling documents as being relevant to Sports or Politics. As before, we will create an artificial (and very small collection) with initial similar steps. \n",
    "\n",
    "There are two modifications for our example to run in a multi-label way:\n",
    "\n",
    "- Change the representation of the data viewing every document as a list of bits, representing being or not to each category. (*MultiLabelBinarizer*)\n",
    "- Run the classifier N times, once for each category where the negative cases will be the documents in all the other categories. (*OneVsRestClassifier*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Artificial (and small) dataset. Sports and Politics\n",
    "train_data = ['Football: a great sport', 'The referee has been very bad this season', 'Our team scored 5 goals', 'I love tenis',\n",
    "              'Politics is in decline in the UK', 'Brexit means Brexit', 'The parlament wants to create new legislation',\n",
    "              'I so want to travel the world', \n",
    "              'The goverment will increase the budget for sports in the UK after the victories in the Olimpic Games',\n",
    "              \"O'Reilly has a great conference this year\"]\n",
    "train_labels = [[\"Sports\"], [\"Sports\"], [\"Sports\"], [\"Sports\"],[\"Politics\"],[\"Politics\"],[\"Politics\"],[],[\"Politics\", \"Sports\"],[]]\n",
    "\n",
    "test_data = ['Swimming is a great sport', \n",
    "             'A lot of policy changes will happen after Brexit', \n",
    "             'The table tenis team will travel to the UK soon for the European Championship',\n",
    "             'The goverment will increase the budget for sports in the UK after the victories in the Olimpic Games',\n",
    "             'PyCon is my favourite conference']\n",
    "test_labels = [[\"Sports\"], [\"Politics\"], [\"Sports\"], [\"Politics\",\"Sports\"],[]]\n",
    "\n",
    "# Change the representation of our data as a list of bit lists \n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_train_labels = mlb.fit_transform(train_labels)\n",
    "binary_test_labels = mlb.transform(test_labels)\n",
    "\n",
    "print(binary_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Represent \n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "vectorised_train_data = vectorizer.fit_transform(train_data)\n",
    "vectorised_test_data = vectorizer.transform(test_data)\n",
    "\n",
    "# One classifer built per category using a one vs the rest approach\n",
    "classifier = OneVsRestClassifier(LinearSVC())\n",
    "classifier.fit(vectorised_train_data, binary_train_labels)\n",
    "\n",
    "#Predict\n",
    "predictions = classifier.predict(vectorised_test_data)\n",
    "\n",
    "print(predictions)\n",
    "print()\n",
    "\n",
    "print(mlb.inverse_transform(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This concludes this notebook!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
